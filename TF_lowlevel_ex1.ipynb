{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise  for using  low level functions in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this exercise is to import data, create train/test datasets, implement a neural network , and train it. TensorFlow includes nice functions to implement many functions in high abtraction level. It is sometimes a good exercise to do more things manually to gain more understanding  what really happens  inside the neural network. Furthermore, if you would like to develop your own functions, such as more advanced gradient descent, you need to be able to do things under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by first downloading the data.  We use here MNIST dataset and we download it from the Yann LeCun webpage (http://yann.lecun.com/exdb/mnist/).  Download the files \"train-images-idx3-ubyte.gz\" and \"train-labels-idx1-ubyte.gz\" in your computer into your working folder and unzip the files if your browser did not do it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do all the needed imports. We need numpy, matplotlib, pylab and tensorflow. The MNIST data is in the IDX file format, we use a tool called \"idx2numpy\" for importing the data.  The idx2numpy imports the data directly to a numpy array. You can install it with pip install idx2numpy. The \"%matplotlib inline\" allows you to print directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do all the needed imports; numpy, matplotlib, tensorflow, pylab, idx2numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import idx2numpy\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to import the data that you have previously downloaded into numpy arrays. If you installed the idx2numpy package, you can use idx2numpy.convert_from_file to import the data.  Import the actual MNIST images and labels into separate numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file for reading\n",
    "# read the data into a numpy array\n",
    "# close the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct the training and test sets.  For this exercise, we do not need all the data. Let us take the first 30000 images from the original training set as our training set, and the following 3000 samples from the original training set as our test set.  For this exercise, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take the samples 1-30000 as a training set\n",
    "# take the samples 30001-33000 as a test set\n",
    "\n",
    "X_train = ???\n",
    "y_train_labels = ???\n",
    "X_test = ???\n",
    "y_test_labels = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us investigate the data. You can observe, that the data values are not between 0 and 1, which might be good for our purposes. Try to investigate what is the range of the values and normalize the data so that the pixel values are between 0 and 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize the pixel value to be between 0 and 1\n",
    "\n",
    "X_train = ???\n",
    "X_test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the data with  e.g. pyplot.  Create a function to plot the first 25 MNIST digits in a nice 5 x 5 picture grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 26 MNISt images into 5 x 5 grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lables are now in a single list of numbers 0-9. For the cross-entropy loss calculations, the labels need to be in onehot format (can you say why?). Construct a simple function to convert  the labels into one-hot format. Make sure your function works with different batch sizes, including a batch size of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct a function to convert list of labels to onehot format\n",
    "\n",
    "def onehot(input):\n",
    "    ???\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function to create the onehot vectors for both the training set and the test set\n",
    "y_train_onehot=onehot(y_train_labels)\n",
    "y_test_onehot=onehot(y_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be ready to start constructing a simple MLP network that we can use for our experiments. We will now take just 300 samples from our training set to keep our computations fast at this point. We will first construct a simple MLP, which takes one vector as an input, so we flatten each MNIST image into one single row vector. All images will be in a single matrix, where the rows correspond to the number of images in the batch size, and the columns represent the input dimensionality. One flattened 28x28 pixel MNIST image has thus shape 1 x 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 300 samples from the training set\n",
    "# flatten each 28x28 image into single row vector and construct the input matrix \n",
    "\n",
    "X_in=X_train[0:300].reshape(-1,784)\n",
    "y_in=y_train_onehot[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensorflow computational graph for an as simple as possible linear mlp network. Only input and output layer and softmax activation. Use cross entropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders\n",
    "x = ???\n",
    "y_ = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Weights and biases\n",
    "W1 = ???\n",
    "b1 = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow computational graph \n",
    "y = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss and training step\n",
    "loss = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session intialization\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "accuracy_values = np.array([])\n",
    "cost_values = np.array([])\n",
    "var_list = [W1, b1]\n",
    "\n",
    "for i in range(50):\n",
    "    #sess.run(train_step, feed_dict={x: X_in, y_: y_in})\n",
    "    loss_train = loss.eval(feed_dict={x: X_in, y_: y_in})\n",
    "    cost_values = np.append(cost_values, loss_train)\n",
    "\n",
    "    gradients = ??? #calculate gradients\n",
    "      \n",
    "    updates = ??? # update gradients\n",
    "    \n",
    "    gradient_descent = tf.group(*updates) # control flow op for simultaneous updates\n",
    "\n",
    "    sess.run(gradient_descent, feed_dict={x: X_in, y_: y_in})\n",
    "    \n",
    "    if i % 10 == 0:        \n",
    "        print 'iteration {}: loss: {}'.format(i, loss_train)\n",
    "\n",
    "\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print'training accuracy',sess.run(accuracy, feed_dict={x: X_in, y_: y_in})\n",
    "print'test accuracy',sess.run(accuracy, feed_dict={x: X_test.reshape(-1,784), y_: y_test_onehot})\n",
    "\n",
    "plt.plot(cost_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let us next try to construct a simple vanilla recurrent neural network in a similar manner. It is a good idea to define all hyperparameters in the beginning to make the program more scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_xdim = 28\n",
    "input_ydim = 28\n",
    "learning_rate = 1e-4\n",
    "hidden_features = 100\n",
    "batch_size = 100\n",
    "classes = 10\n",
    "iterations = 1000 # Must be at least 10k to obtain over 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the placeholders for the input data. This time the placeholder for the input image needs to be a tensor of rank 3, as we will feed one row of the image at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_rows = tf.placeholder(tf.float32, [None, input_xdim, input_ydim])\n",
    "labels = tf.placeholder(tf.float32, [None, classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define the trainable variables. In each RNN cell we need one weight matrix W for the hidden vector passed on from the previous cell, one weight matrix U for the actual input to the cell and a bias b1. We will also need a weight matrix V for the output layer from each cell and a bias b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([hidden_features, hidden_features], stddev=0.1))\n",
    "U = tf.Variable(tf.truncated_normal([input_xdim, hidden_features], stddev=0.1))\n",
    "V = tf.Variable(tf.truncated_normal([hidden_features, classes], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[1, hidden_features]))\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[1, classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to have a function available for constructing the output layers. So let's define a function that takes as parameters an input and an activation function. If the function is called to create an output layer, it can be called with softmax as the activation. Use the above defined variables V and b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(inputs, activation):\n",
    "    outputs = ???\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we make a very simple function for creating a vanilla RNN cell. It takes as parameters a particular row of the image and a hidden state. The output should be the new hidden state. Use the above defined variables W, U and b1, and tanh as activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(inputs, hidden):\n",
    "    hidden_out = ???\n",
    "    return hidden_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next function we implement the actual RNN network. There is one RNN cell for each row, and each cell outputs a hidden vector, and an output state which is the hidden vector passed through a linear output layer with softmax. Average across all softmax outputs to get final prediction. There are many ways to do it, feel free to try out your own implementation. You can alternatively try to fill in the two incomplete parts in the function below. The only thing you have to do is to call the two previous functions with the right parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs):\n",
    "    rows = tf.unstack(inputs, input_ydim, 1) # Make an array of 2D tensors\n",
    "    samples = tf.shape(rows)[1] # Number of samples in the batch/test set\n",
    "    hidden_vec = tf.zeros([samples, hidden_features]) # Here we will store the hidden output from each cell\n",
    "    for cell_index in range(input_ydim): # Iterate through all cells\n",
    "        current = rows[cell_index] # Get the current row\n",
    "        hidden_vec = ??? # Compute the hidden output from the current cell \n",
    "        cell_output = ??? # Compute the final output from the current cell\n",
    "        if cell_index == 0:\n",
    "            out = cell_output\n",
    "        else:\n",
    "            out = out + cell_output # Add together final outputs of all cells\n",
    "    return tf.multiply(1.0/input_ydim, out) # and divide to get average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we are ready to get the prediction and calculate the cost function. For the prediction, we just call the rnn function and for the cost function write your own cross entropy function, i.e. multiply onehot label with the negative logarithm of the prediction, sum over all classes and average over all samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = rnn(input_rows)\n",
    "cost_function = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grads = tf.gradients(cost_function, [V, b2, U, W, b1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next try to implement your own version of gradient descent. tf.assign takes as the first parameter the variable to be updated as a second parameter the updated variable. The updated variable is simply the original variable minus learning rate multiplied by the gradient for the respective variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = [tf.assign(???),\n",
    "                   tf.assign(???),\n",
    "                   tf.assign(???),\n",
    "                   tf.assign(???),\n",
    "                   tf.assign(???)]\n",
    "\n",
    "gradient_descent = tf.group(*updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = tf.argmax(labels, axis = 1)\n",
    "predicted_labels = tf.argmax(score, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate = tf.equal(true_labels, predicted_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(evaluate, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image = X_train.reshape(30000, input_ydim, input_xdim)\n",
    "test_image = X_test.reshape(3000, input_ydim, input_xdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = np.zeros(iterations/100, float)\n",
    "train_acc = np.zeros(iterations/100, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    start = (i * batch_size) % (30000 - (2 * batch_size))\n",
    "    end = start + batch_size\n",
    "    batch_image = X_train[start:end, :, :]\n",
    "    batch_label = y_train_onehot[start:end, :]\n",
    "    sess.run(gradient_descent, feed_dict={input_rows: batch_image, labels: batch_label})\n",
    "    if i % 100 == 0:  \n",
    "        print 'iteration {}: '.format(i)\n",
    "print(\"The test accuracy is %g\" %accuracy.eval(feed_dict={input_rows: test_image, labels: y_test_onehot}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of the exercise is to visualize the results for the rnn network. We will plot the training loss and accuracy, and draw a confusion matrix to see how the true and predicted classes relate to one another. (The main goal here is to practise visulizations with matplotlib, keep in mind that rnn's are not ideal for learning mnist.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = ??? # use tf.confusion_matrix\n",
    "cm = confusion_matrix.eval(feed_dict={input_rows: test_image, labels: y_test_onehot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "iters = range(len(train_loss)) # x values values for plots\n",
    "# Use matplotlib to plot training cost and accuracy\n",
    "???\n",
    "plt.draw()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "# Use matplotlib to draw confusion matrix\n",
    "???\n",
    "plt.draw()\n",
    "sess.close()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
